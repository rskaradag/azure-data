{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, expr\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.keyvault.secrets import SecretClient\n",
        "\n",
        "keyvault_url = \"https://kv-rabo.vault.azure.net/\"\n",
        "credential = DefaultAzureCredential()\n",
        "client = SecretClient(vault_url=keyvault_url, credential=credential)\n",
        "sql_server = client.get_secret(\"SQL-SERVER\")\n",
        "sql_database = client.get_secret(\"SQL-DATABASE\")\n",
        "sql_user = client.get_secret(\"SQL-USER\")\n",
        "sql_password = client.get_secret(\"SQL-PASSWORD\")\n",
        "\n",
        "jdbc_url = f\"jdbc:sqlserver://{sql_server}.database.windows.net:1433;\" \\\n",
        "           f\"database={sql_database};\" \\\n",
        "           f\"user={sql_user}@{sql_server};\" \\\n",
        "           f\"password={sql_password};\" \\\n",
        "           \"encrypt=true;trustServerCertificate=false;\" \\\n",
        "           \"hostNameInCertificate=*.database.windows.net;loginTimeout=30;\"\n",
        "\n",
        "\n",
        "# 1. Read the CSV file from Azure Data Lake Storage Gen2\n",
        "df = spark.read.option(\"header\", True).csv(\"abfss://cnt-rabodata-dev-northeurope@strabodatadevnortheurope.dfs.core.windows.net/transactions.csv\")\n",
        "\n",
        "# 2. Convert column types\n",
        "df = df.withColumn(\"Reference\", col(\"Reference\").cast(\"long\")) \\\n",
        "       .withColumn(\"Start Balance\", col(\"Start Balance\").cast(\"double\")) \\\n",
        "       .withColumn(\"Mutation\", col(\"Mutation\").cast(\"double\")) \\\n",
        "       .withColumn(\"End Balance\", col(\"End Balance\").cast(\"double\"))\n",
        "\n",
        "# 3. Calculate the expected end balance\n",
        "df = df.withColumn(\"Computed_End\", expr(\"`Start Balance` + Mutation\"))\n",
        "\n",
        "# 4. Filter valid records (unique reference and matching end balance)\n",
        "valid_df = df.dropDuplicates([\"Reference\"]) \\\n",
        "             .filter(col(\"End Balance\") == col(\"Computed_End\"))\n",
        "\n",
        "# 5. Identify invalid records\n",
        "invalid_df = df.subtract(valid_df)\n",
        "\n",
        "# 6. Write invalid records (Reference and Description) to a separate folder\n",
        "invalid_df.select(\"Reference\", \"Description\") \\\n",
        "          .write.mode(\"overwrite\") \\\n",
        "          .option(\"header\", True) \\\n",
        "          .csv(\"abfss://cnt-rabodata-dev-northeurope@strabodatadevnortheurope.dfs.core.windows.net/output/invalid\")\n",
        "\n",
        "# (Optional) Display valid records\n",
        "valid_df.show()\n",
        "\n",
        "valid_df.write \\\n",
        "    .format(\"jdbc\") \\\n",
        "    .option(\"url\", jdbc_url) \\\n",
        "    .option(\"dbtable\", \"validated_transactions\") \\\n",
        "    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .save()"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
